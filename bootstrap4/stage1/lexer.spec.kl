use .lexer.Lexer
use .lexer.Token
use .lexer.TokenKind
use .lexer.FStrItem
use .lexer.Span
use .lexer.to_src
use .test.Test

use TokenKind.*
use FStrItem.*

fn main():
    mut test = Test.new_from_args("lexer", env.args)

    test.it("lexes identifiers", fn() throws:
        lex_run("x", [Ident("x")])
        lex_run("x y", [Ident("x"), Ident("y")])
        lex_run("x Y _ a1 _a1 A1 _A1 a_1", [
            Ident("x"), Ident("Y"), Ident("_"), Ident("a1"), Ident("_a1"), 
            Ident("A1"), Ident("_A1"), Ident("a_1")
        ])
    end)

    test.it("lexes comments and minus (-)", fn() throws:
        lex_run("-- comment", [Comment("-- comment")])
        lex_run("--- multi-line\ncomment --- x", [Comment("--- multi-line\ncomment ---"), Ident("x")])
        -- todo (lang-feat): Multi-line strings should be dedented automatically and have to option to be raw.
        lex_run("""
--- Multi-line comment 
- with a dash
-- and a double dash
---
x
            """,
            [Comment("--- Multi-line comment \n- with a dash\n-- and a double dash\n---"), Ident("x")]
        )
        lex_run("- -- comment", [Minus, Comment("-- comment")])
    end)

    test.it("lexes operators and brackets", fn() throws:
        lex_run("= == =>", [Equal, DoubleEqual, FatArrow])
        lex_run("< <=", [LessThan, LessThanOrEqual])
        lex_run("> >=", [GreaterThan, GreaterThanOrEqual])
        lex_run("+ * /", [Plus, Star, Slash])
        lex_run("! !=", [Exclamation, NotEqual])
        lex_run("( ) { } [ ]", [LParen, RParen, LBrace, RBrace, LBracket, RBracket])
    end)

    test.it("lexes int literals", fn() throws:
        lex_run("123 12 -42", [IntLiteral("123"), IntLiteral("12"), IntLiteral("-42")])
    end)

    test.it("lexes all keywords", fn() throws:
        lex_run(
            """
            and break continue else end enum extern false fn for if impl 
            let loop match mut not or return struct throws trait true use
            """, [
            And, Break, Continue, Else, End, Enum, Extern, BoolLiteral(false), Fn, For, If, Impl, 
            Let, Loop, Match, Mut, Not, Or, Return, Struct, Throws, Trait, BoolLiteral(true), Use
        ])
    end)

    test.it("records correct spans", fn() throws:
        fn test_span(src Str, expected_from Int, expected_to Int, expected_str Str):
            let tokens = lex_only(src)
            -- todo (lang-feat): We should be able to use negative indexes.
            let i = tokens.len() - 1
            assert(tokens[i].span.from == expected_from)
            assert(tokens[i].span.to == expected_to)
            assert(tokens[i].span.to_str() == expected_str)
        end
        test_span("x", 0, 0, "test.kl:1:1")
        test_span("\nx", 1, 1, "test.kl:2:1")
        test_span("\n   for", 4, 6, "test.kl:2:4-6")
    end)

    test.group("char literals", fn():
        test.it("passes the happy path", fn() throws:
            lex_run("'a'", [CharLiteral('a')])
        end)

        test.it("lexes escape sequences", fn() throws:
            lex_run("'\\0'", [CharLiteral('\0')])
            lex_run("'\\n'", [CharLiteral('\n')])
            lex_run("'\\r'", [CharLiteral('\r')])
            lex_run("'\\t'", [CharLiteral('\t')])
            lex_run("'\\\\'", [CharLiteral('\\')])
            lex_run("'\\''", [CharLiteral('\'')])
            lex_run("'\\x41'", [CharLiteral('A')])
            lex_run("'\\u0041'", [CharLiteral('A')])
        end)

        test.it("detects invalid char literals and recovers", fn() throws:
            lex_run("'\\l' foo", [LexError("invalid escape sequence"), Ident("foo")])
            lex_run("'\\x1' foo", [LexError("invalid escape sequence"), Ident("foo")])
            lex_run("'\\x1g' foo", [LexError("invalid escape sequence"), Ident("foo")])
            lex_run("'' foo", [LexError("empty character literal"), Ident("foo")])
            lex_run("'\\u123' foo", [LexError("invalid escape sequence"), Ident("foo")])
        end)

        test.it("detects unterminated char literals and recovers", fn() throws:
            lex_run("'a", [LexError("expected closing '")])
            lex_run("'a\n foo", [LexError("expected closing '"), Ident("foo")])
        end)

    end)

    test.group("string literals", fn():
        test.it("lexes string literals", fn() throws:
            lex_run("\"hello\"", [StrLiteral("hello", false)])
            -- Note: We can't distinguish between `\n` and `\\n` in the source code.
            --       That's not a shortcoming of the lexer, but the nature of things.
            lex_run("\"\"\"hello\n\\nworld\"\"\"", [StrLiteral("hello\n\nworld", true)])
            lex_run("\"escape \\n \\\"\"", [StrLiteral("escape \n \"", false)])
        end)

        test.it("lexes escape sequences", fn() throws:
            lex_run("\"\\0\"", [StrLiteral("\0", false)])
            lex_run("\"\\n\"", [StrLiteral("\n", false)])
            lex_run("\"\\r\"", [StrLiteral("\r", false)])
            lex_run("\"\\t\"", [StrLiteral("\t", false)])
            lex_run("\"\\\\\"", [StrLiteral("\\", false)])
            lex_run("\"\\\"\"", [StrLiteral("\"", false)])
            lex_run("\"\\x41\"", [StrLiteral("A", false)])
            lex_run("\"\\u0041\"", [StrLiteral("A", false)])
        end)

        test.it("detects unterminated strings and recovers", fn() throws:
            lex_run("\"string literal eof", [LexError("unexpected end of file")])
            lex_run("\"string literal \nfoo", [LexError("unexpected end of line"), Ident("foo")])
            lex_run("\"\"\"string literal eof", [LexError("unexpected end of file")])
        end)
    end)

    test.group("f-strings", fn():
        test.it("lexes single-line f-strings", fn() throws:
            let f_str = lex_only("f\"the {question.answer} is {42}\"")
            assert(f_str.len() == 1)
            match f_str[0].kind:
                FStr(items, is_multi_line):
                    assert(is_multi_line == false)
                    assert(items.len() == 4)
                    match items[0]:
                        FStrStrItem(s) => assert(s == "the ")
                        _ => return Error("expected FStrStrItem.")
                    end
                    match items[1]:
                        FStrTokensItem(tokens):
                            let kinds = tokens.iter().map<TokenKind>(
                                fn(token Token) => token.kind).collect()
                            assert(kinds == Vector<TokenKind>.from(
                                [Ident("question"), Dot, Ident("answer")])
                            )
                        end
                        _ => return Error("expected FStrTokensItem.")
                    end
                    match items[2]:
                        FStrStrItem(s) => assert(s == " is ")
                        _ => return Error("expected FStrStrItem.")
                    end
                    match items[3]:
                        FStrTokensItem(tokens):
                            let kinds = tokens.iter().map<TokenKind>(
                                fn(token Token) => token.kind).collect()
                            assert(kinds == Vector<TokenKind>.from([IntLiteral("42")]))
                        end
                        _ => return Error("expected FStrTokensItem.")
                    end
                end
                _ => return Error("expected FStr.")
            end
        end)

        test.it("lexes multi-line f-strings", fn() throws:
            let f_str_multi_line = lex_only("f\"\"\"the \n{question.\nanswer} is {42}\"\"\"")
            assert(f_str_multi_line.len() == 1)
            match f_str_multi_line[0].kind:
                FStr(items, is_multi_line):
                    assert(is_multi_line == true)
                    assert(items.len() == 4)
                    match items[0]:
                        FStrStrItem(s) => assert(s == "the \n")
                        _ => return Error("expected FStrStrItem.")
                    end
                    match items[1]:
                        FStrTokensItem(tokens):
                            let kinds = tokens.iter().map<TokenKind>(
                                fn(token Token) => token.kind).collect()
                            assert(kinds == Vector<TokenKind>.from(
                                [Ident("question"), Dot, WhiteSpace('\n'), Ident("answer")])
                            )
                        end
                        _ => Error("expected FStrTokensItem.")
                    end
                    match items[2]:
                        FStrStrItem(s) => assert(s == " is ")
                        _ => return Error("expected FStrStrItem.")
                    end
                    match items[3]:
                        FStrTokensItem(tokens):
                            let kinds = tokens.iter().map<TokenKind>(
                                fn(token Token) => token.kind).collect()
                            assert(kinds == Vector<TokenKind>.from([IntLiteral("42")]))
                        end
                        _ => return Error("expected FStrTokensItem.")
                    end
                end
                _ => return Error("expected FStr.")
            end
        end)

        test.it("lexes escape sequences", fn() throws:
            fn test_escape(src Str, expected_str Str) throws:
                let res = lex_only(src) 
                assert(res.len() == 1)
                match res[0].kind:
                    FStr(items, _):
                        assert(items.len() == 1)
                        match items[0]:
                            FStrStrItem(s) => assert(s == expected_str)
                            _ => return Error("expected FStrStrItem.")
                        end
                    end
                    _ => return Error("expected FStr.")
                end
            end
            test_escape("f\"\\n\"", "\n")!
            test_escape("f\"\\r\"", "\r")!
            test_escape("f\"\\t\"", "\t")!
            test_escape("f\"\\\\\"", "\\")!
            test_escape("f\"\\\"\"", "\"")!
            test_escape("f\"\\x41\"", "A")!
            test_escape("f\"\\u0041\"", "A")!
        end)

        test.it("detects unterminated f-strings and recovers", fn() throws:
            lex_run("f\"string literal eof", [LexError("unexpected end of file")])
            lex_run("f\"string literal \nfoo", [LexError("unexpected end of line"), Ident("foo")])
            lex_run("f\"\"\"string literal eof", [LexError("unexpected end of file")])
        end)
    end)

    fn lex_only(src Str) Vector<Token> => Lexer.lex(src, "test.kl").collect()

    fn lex_run(src Str, expected Array<TokenKind>):
        --- Test that `src` is lexed into `expected`. Whitespaces are ignored.
            Test that `lex()` and `to_src()` are inverses of each other.
        ---
        test.log(f"testing: {src}")
        let tokens = lex_only(src)
        mut idx = 0
        mut has_expected_parse_errors = expected.iter().any(
            fn(kind TokenKind) Bool => match(kind):
                LexError(_) => true
                _ => false
            end
        )
        tokens.iter().filter(
            fn(token Token) Bool => match token.kind:
                WhiteSpace(_) => false
                _ => true
            end
        ).for_each(
            fn(token Token): 
                if idx >= expected.len():
                    panic(f"more tokens than expected: {token}")
                end
                assert(idx < expected.len())
                assert(token.kind == expected[idx]) 
                idx = idx + 1
            end
        )
        if idx < expected.len():
            panic(f"less tokens than expected: {expected[idx]}")
        end
        if not has_expected_parse_errors => assert(to_src(tokens) == src)
    end

    test.run_and_exit()
end
