use .lexer::Lexer
use .lexer::Token
use .lexer::TokenKind
use .lexer::FStrItem
use .lexer::Span
use .lexer::to_src
use .test::Test

use TokenKind::*
use FStrItem::*

fn main():
    mut test = Test::new_from_args("lexer", env.args)

    test.it("lexes identifiers", fn() throws:
        lex_run("x", [Ident("x")])
        lex_run("x y", [Ident("x"), Ident("y")])
        lex_run("x Y a1 _a1 A1 _A1 a_1", [
            Ident("x"), Ident("Y"), Ident("a1"), Ident("_a1"), 
            Ident("A1"), Ident("_A1"), Ident("a_1")
        ])
    end)

    test.it("lexes comments and minus (-)", fn() throws:
        lex_run("-- comment", [Comment("-- comment")])
        lex_run("--- multi-line\ncomment --- x", [Comment("--- multi-line\ncomment ---"), Ident("x")])
        -- todo (lang-feat): Multi-line strings should be dedented automatically and have to option to be raw.
        lex_run("""
--- Multi-line comment 
- with a dash
-- and a double dash
---
x
            """,
            [Comment("--- Multi-line comment \n- with a dash\n-- and a double dash\n---"), Ident("x")]
        )
        lex_run("- -- comment", [Minus, Comment("-- comment")])
    end)

    test.it("lexes operators and brackets", fn() throws:
        lex_run("= == =>", [Equal, DoubleEqual, FatArrow])
        lex_run("< <=", [LessThan, LessThanOrEqual])
        lex_run("> >=", [GreaterThan, GreaterThanOrEqual])
        lex_run("+ * / %", [Plus, Star, Slash, Percent])
        lex_run("! !=", [Exclamation, NotEqual])
        lex_run("( ) { } [ ]", [LParen, RParen, LBrace, RBrace, LBracket, RBracket])
    end)

    test.it("lexes int literals", fn() throws:
        lex_run("123 12 -42", [IntLiteral("123"), IntLiteral("12"), IntLiteral("-42")])
    end)

    test.it("lexes punctuation", fn() throws:
        lex_run(", . : ; :: ? !", [Comma, Dot, Colon, Semicolon, DoubleColon, QMark, Exclamation])
    end)

    test.it("lexes underscore", fn() throws:
        lex_run("_", [Underscore])
    end)

    test.it("lexes all keywords", fn() throws:
        lex_run(
                """
            and break continue else end enum extern false fn for if impl 
            let loop match mut not or return struct throws trait true use
            """, [
            And, Break, Continue, Else, End, Enum, Extern, BoolLiteral(false), Fn, For, If, Impl, 
            Let, Loop, Match, Mut, Not, Or, Return, Struct, Throws, Trait, BoolLiteral(true), Use
        ])
    end)

    test.it("records correct spans", fn() throws:
        fn test_span(src Str, expected_from Int, expected_to Int, expected_str Str):
            let tokens = lex_only(src)
            -- todo (lang-feat): We should be able to use negative indexes.
            let i = tokens.len() - 1
            assert(tokens[i].span.from == expected_from)
            assert(tokens[i].span.to == expected_to)
            assert(tokens[i].span.to_str() == expected_str)
        end
        test_span("x", 0, 0, "test.kl:1:1")
        test_span("\nx", 1, 1, "test.kl:2:1")
        test_span("\n   for", 4, 6, "test.kl:2:4-6")
    end)

    test.group("char literals", fn():
        test.it("passes the happy path", fn() throws:
            lex_run("'a'", [CharLiteral('a')])
        end)

        test.it("lexes escape sequences", fn() throws:
            lex_run("'\\0'", [CharLiteral('\0')])
            lex_run("'\\n'", [CharLiteral('\n')])
            lex_run("'\\r'", [CharLiteral('\r')])
            lex_run("'\\t'", [CharLiteral('\t')])
            lex_run("'\\\\'", [CharLiteral('\\')])
            lex_run("'\\{'", [CharLiteral('{')])
            lex_run("'\\}'", [CharLiteral('}')])
            lex_run("'\\''", [CharLiteral('\'')])
            lex_run("'\\x41'", [CharLiteral('A')])
            lex_run("'\\u0041'", [CharLiteral('A')])
        end)

        test.it("detects invalid char literals and recovers", fn() throws:
            lex_run("'\\l' foo", [LexError("invalid escape sequence"), Ident("foo")])
            lex_run("'\\x1' foo", [LexError("invalid escape sequence"), Ident("foo")])
            lex_run("'\\x1g' foo", [LexError("invalid escape sequence"), Ident("foo")])
            lex_run("'' foo", [LexError("empty character literal"), Ident("foo")])
            lex_run("'\\u123' foo", [LexError("invalid escape sequence"), Ident("foo")])
        end)

        test.it("detects unterminated char literals and recovers", fn() throws:
            lex_run("'a", [LexError("expected closing '")])
            lex_run("'a\n foo", [LexError("expected closing '"), Ident("foo")])
        end)

    end)

    test.group("string literals", fn():
        test.it("lexes string literals", fn() throws:
            lex_run("\"hello\"", [StrLiteral("hello", false)])
            -- Note: We can't distinguish between `\n` and `\\n` in the source code.
            --       That's not a shortcoming of the lexer, but the nature of things.
            lex_run("\"\"\"hello\n\\nworld\"\"\"", [StrLiteral("hello\n\nworld", true)])
            lex_run("\"escape \\n \\\"\"", [StrLiteral("escape \n \"", false)])
        end)

        test.it("lexes escape sequences", fn() throws:
            lex_run("\"\\0\"", [StrLiteral("\0", false)])
            lex_run("\"\\n\"", [StrLiteral("\n", false)])
            lex_run("\"\\r\"", [StrLiteral("\r", false)])
            lex_run("\"\\t\"", [StrLiteral("\t", false)])
            lex_run("\"\\\\\"", [StrLiteral("\\", false)])
            lex_run("\"\\\"\"", [StrLiteral("\"", false)])
            lex_run("\"\\x41\"", [StrLiteral("A", false)])
            lex_run("\"\\u0041\"", [StrLiteral("A", false)])
        end)

        test.it("detects unterminated strings and recovers", fn() throws:
            lex_run("\"string literal eof", [LexError("unexpected end of file")])
            lex_run("\"string literal \nfoo", [LexError("unexpected end of line"), Ident("foo")])
            lex_run("\"\"\"string literal eof", [LexError("unexpected end of file")])
        end)

        test.it("rejects the null character", fn() throws:
            assert(lex_error("\"\0\"") == "control character is illegal")
            -- But escaped null characters are fine.
            lex_run("\"\\0\"", [StrLiteral("\0", false)]) 
        end)

        mut i = 1
        loop:
            -- todo (lang-feat): We should be able to specify formatting options in f-strings.
            mut formatter = StrFormatter::new()
            formatter.radix = Some(16)
            formatter.width = Some(4)
            i.to_formatted_str(formatter).unwrap()
            let unicode_escape = f"\\u{formatter}"
            let c = Char::from_int(i).unwrap()

            if i == 9 or i == 10 or i == 13:
                test.it(f"parses the control `{unicode_escape}` character", fn() throws:
                    lex_run(f"\"\"\"{c}\"\"\"", [StrLiteral(f"""{c}""", true)])
                end)
            end else:

                test.it(f"rejects the control `{unicode_escape}` character", fn() throws:
                    assert(lex_error(f"\"\"\"{c}\"\"\"") == "control character is illegal")
                    -- But escaped characters are fine.
                    lex_run(f"\"{unicode_escape}\"", [StrLiteral(f"{c}", false)]) 
                end)

            end

            i = i + 1
            if i > 31:
                break
            end
        end
    end)

    test.group("f-strings", fn():
        test.it("lexes single-line f-strings", fn() throws:
            let f_str = lex_only("f\"the {question.answer} is {42}\"")
            assert(f_str.len() == 1)
            match f_str[0].kind:
                FStr(items, is_multi_line):
                    assert(is_multi_line == false)
                    assert(items.len() == 4)
                    match items[0]:
                        FStrStrItem(s, span):
                            assert(s == "the ")
                            assert(span.raw_src() == "the ")
                        end
                        _ => return Error("expected FStrStrItem.")
                    end
                    match items[1]:
                        FStrTokensItem(tokens, span):
                            let kinds = tokens.iter().map<TokenKind>(
                                fn(token Token) => token.kind).collect()
                            assert(kinds == Vector<TokenKind>::from(
                                [Ident("question"), Dot, Ident("answer")])
                            )
                            assert(span.raw_src() == "{question.answer}")
                        end
                        _ => return Error("expected FStrTokensItem.")
                    end
                    match items[2]:
                        FStrStrItem(s, span):
                            assert(s == " is ")
                            assert(span.raw_src() == " is ")
                        end
                        _ => return Error("expected FStrStrItem.")
                    end
                    match items[3]:
                        FStrTokensItem(tokens, span):
                            let kinds = tokens.iter().map<TokenKind>(
                                fn(token Token) => token.kind).collect()
                            assert(kinds == Vector<TokenKind>::from([IntLiteral("42")]))
                            assert(span.raw_src() == "{42}")
                        end
                        _ => return Error("expected FStrTokensItem.")
                    end
                end
                _ => return Error("expected FStr.")
            end
        end)

        test.it("lexes multi-line f-strings", fn() throws:
            let f_str_multi_line = lex_only("f\"\"\"the \n{question.\nanswer} is {42}\"\"\"")
            assert(f_str_multi_line.len() == 1)
            match f_str_multi_line[0].kind:
                FStr(items, is_multi_line):
                    assert(is_multi_line == true)
                    assert(items.len() == 4)
                    match items[0]:
                        FStrStrItem(s, span):
                            assert(s == "the \n")
                            assert(span.raw_src() == "the \n")
                        end
                        _ => return Error("expected FStrStrItem.")
                    end
                    match items[1]:
                        FStrTokensItem(tokens, span):
                            let kinds = tokens.iter().map<TokenKind>(
                                fn(token Token) => token.kind).collect()
                            assert(kinds == Vector<TokenKind>::from(
                                [Ident("question"), Dot, WhiteSpace('\n'), Ident("answer")])
                            )
                            assert(span.raw_src() == "{question.\nanswer}")
                        end
                        _ => Error("expected FStrTokensItem.")
                    end
                    match items[2]:
                        FStrStrItem(s, span):
                            assert(s == " is ")
                            assert(span.raw_src() == " is ")
                        end
                        _ => return Error("expected FStrStrItem.")
                    end
                    match items[3]:
                        FStrTokensItem(tokens, span):
                            let kinds = tokens.iter().map<TokenKind>(
                                fn(token Token) => token.kind).collect()
                            assert(kinds == Vector<TokenKind>::from([IntLiteral("42")]))
                            assert(span.raw_src() == "{42}")
                        end
                        _ => return Error("expected FStrTokensItem.")
                    end
                end
                _ => return Error("expected FStr.")
            end
        end)

        test.it("lexes escape sequences", fn() throws:
            fn test_escape(src Str, expected_str Str, expected_raw_str Str) throws:
                let res = lex_only(src)
                assert(res.len() == 1)
                match res[0].kind:
                    FStr(items, _):
                        assert(items.len() == 1)
                        match items[0]:
                            FStrStrItem(s, span):
                                assert(s == expected_str)
                                assert(span.raw_src() == expected_raw_str)
                            end
                            _ => return Error("expected FStrStrItem.")
                        end
                    end
                    _ => return Error("expected FStr.")
                end
            end
            test_escape("f\"\\n\"", "\n", "\\n")!
            test_escape("f\"\\r\"", "\r", "\\r")!
            test_escape("f\"\\t\"", "\t", "\\t")!
            test_escape("f\"\\{\"", "\{", "\\{")!
            test_escape("f\"\\}\"", "\}", "\\}")!
            test_escape("f\"\\\\\"", "\\", "\\\\")!
            test_escape("f\"\\\"\"", "\"", "\\\"")!
            test_escape("f\"\\x41\"", "A", "\\x41")!
            test_escape("f\"\\u0041\"", "A", "\\u0041")!
        end)

        test.it("detects unterminated f-strings and recovers", fn() throws:
            lex_run("f\"string literal eof", [LexError("unexpected end of file")])
            lex_run("f\"string literal \nfoo", [LexError("unexpected end of line"), Ident("foo")])
            lex_run("f\"\"\"string literal eof", [LexError("unexpected end of file")])
        end)
    end)

    fn lex_only(src Str) Vector<Token> => Lexer::lex(src, "test.kl").collect()

    fn lex_error(src Str) Str:
        let tokens = lex_only(src)
        assert(tokens.len() >= 1)
        let first_error = tokens.iter().find_map<Str>(
            fn(token Token) => match token.kind:
                LexError(msg) => Some(msg)
                _ => None
            end
        )
        match first_error:
            Some(msg) => msg
            None => panic("expected LexError")
        end
    end

    fn lex_run(src Str, expected Array<TokenKind>):
        --- Test that `src` is lexed into `expected`. Whitespaces are ignored.
        Test that `lex()` and `to_src()` are inverses of each other.
        ---
        test.log(f"testing: {src}")
        let tokens = lex_only(src)
        mut idx = 0
        mut has_expected_parse_errors = expected.iter().any(
            fn(kind TokenKind) Bool => match(kind):
                LexError(_) => true
                _ => false
            end
        )
        tokens.iter().filter(
            fn(token Token) Bool => match token.kind:
                WhiteSpace(_) => false
                _ => true
            end
        ).for_each(
            fn(token Token): 
                if idx >= expected.len():
                    panic(f"more tokens than expected: {token}")
                end
                assert(idx < expected.len())
                assert(token.kind == expected[idx])
                idx = idx + 1
            end
        )
        if idx < expected.len():
            panic(f"less tokens than expected: {expected[idx]}")
        end
        if not has_expected_parse_errors => assert(to_src(tokens) == src)
    end

    test.run_and_exit()
end
